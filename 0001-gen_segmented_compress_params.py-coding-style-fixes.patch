From 892047639f2f86b018f383bbdbecc2d1bad22bd9 Mon Sep 17 00:00:00 2001
From: Alexys Jacob <ultrabug@gentoo.org>
Date: Sun, 4 Nov 2018 12:57:24 +0100
Subject: [PATCH] gen_segmented_compress_params.py: coding style fixes

gen_segmented_compress_params.py:52:47: E226 missing whitespace around
arithmetic operator
gen_segmented_compress_params.py:56:64: E226 missing whitespace around
arithmetic operator
gen_segmented_compress_params.py:60:36: E226 missing whitespace around
arithmetic operator
gen_segmented_compress_params.py:60:48: E226 missing whitespace around
arithmetic operator
gen_segmented_compress_params.py:70:35: E226 missing whitespace around
arithmetic operator
gen_segmented_compress_params.py:70:48: E226 missing whitespace around
arithmetic operator
gen_segmented_compress_params.py:99:43: E226 missing whitespace around
arithmetic operator
gen_segmented_compress_params.py:106:18: E225 missing whitespace around
operator
gen_segmented_compress_params.py:120:5: E303 too many blank lines (2)
gen_segmented_compress_params.py:200:30: E261 at least two spaces before
inline comment
gen_segmented_compress_params.py:200:31: E262 inline comment should start with
'# '
gen_segmented_compress_params.py:218:76: E261 at least two spaces before
inline comment
gen_segmented_compress_params.py:219:59: E703 statement ends with a semicolon
gen_segmented_compress_params.py:219:60: E261 at least two spaces before
inline comment

Signed-off-by: Alexys Jacob <ultrabug@gentoo.org>
---
 gen_segmented_compress_params.py | 19 +++++++++----------
 1 file changed, 9 insertions(+), 10 deletions(-)

diff --git a/gen_segmented_compress_params.py b/gen_segmented_compress_params.py
index 98c4e6a24..5b85a8da2 100755
--- a/gen_segmented_compress_params.py
+++ b/gen_segmented_compress_params.py
@@ -49,15 +49,15 @@ def relative_offset_size(data_size, chunk_size, n):
     if n == 1:
         return int(0)
     else:
-        return int(math.ceil(math.log2((n - 1)*(chunk_size + 64))))
+        return int(math.ceil(math.log2((n - 1) * (chunk_size + 64))))
 
 
 def segment_size(data_size, chunk_size, n):
-    return base_offset_size(data_size, chunk_size, n) + (n - 1)*relative_offset_size(data_size, chunk_size, n)
+    return base_offset_size(data_size, chunk_size, n) + (n - 1) * relative_offset_size(data_size, chunk_size, n)
 
 
 def no_of_segments(data_size, chunk_size, n):
-    return int(math.ceil((data_size/chunk_size)/n))
+    return int(math.ceil((data_size / chunk_size) / n))
 
 
 def n_for(data_size, chunk_size, n_values):
@@ -67,7 +67,7 @@ def n_for(data_size, chunk_size, n_values):
 
 
 def size_deque(data_size, chunk_size):
-    return int(math.ceil(data_size/chunk_size))*64
+    return int(math.ceil(data_size / chunk_size)) * 64
 
 
 def size_grouped_segments(data_size, chunk_size, n):
@@ -96,14 +96,14 @@ def segments_per_bucket(data_size, chunk_size, n, bucket_size):
     bucket_size_bits = bucket_size * 8 - 56
     segment_size_bits = segment_size(data_size, chunk_size, n)
 
-    fits = int(math.floor(bucket_size_bits/segment_size_bits))
+    fits = int(math.floor(bucket_size_bits / segment_size_bits))
 
     # We can't have more segments than the sizes support
     return min(no_of_segments(data_size, chunk_size, n), fits)
 
 
 def all_n_values():
-    optimal_sizes={}
+    optimal_sizes = {}
 
     for f in data_size_range_log2():
         for c in chunk_size_range_log2():
@@ -116,7 +116,6 @@ def all_n_values():
 
             optimal_sizes[(f, c)] = optimal_size
 
-
     n_values = []
     for k in sorted(optimal_sizes.keys()):
         f, c, n, s = optimal_sizes[k]
@@ -197,7 +196,7 @@ if __name__ == '__main__':
             print("Bucket size is either too large or too small")
             exit(1)
     else:
-        bucket_size_log2 = 12 #4K
+        bucket_size_log2 = 12  # 4K
 
     bucket_size = 2**bucket_size_log2
 
@@ -215,8 +214,8 @@ if __name__ == '__main__':
             bucket_infos.append("    {{{}, {}, {} /*out of the max of {}*/}}".format(
                 chunk_size_log2,
                 data_size_log2,
-                segments_per_bucket(data_size, chunk_size, n, bucket_size), # no of segments that fit into the bucket
-                no_of_segments(data_size, chunk_size, n))); # normal no of segments for these sizes
+                segments_per_bucket(data_size, chunk_size, n, bucket_size),  # no of segments that fit into the bucket
+                no_of_segments(data_size, chunk_size, n)))  # normal no of segments for these sizes
             data_sizes.append(data_size_log2)
 
         segment_infos = []
-- 
2.19.0

